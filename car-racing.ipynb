{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokyoDrifter(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TokyoDrifter, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(256, (4, 4), activation='relu', input_shape=(96,96,3))\n",
    "        self.pool1 = layers.MaxPool2D(2,2)\n",
    "        self.conv2 = layers.Conv2D(256, (4, 4), activation='relu')\n",
    "        self.pool2 = layers.MaxPool2D(2,2)\n",
    "        self.conv3 = layers.Conv2D(256, (4, 4), activation='relu')\n",
    "        self.pool3 = layers.MaxPool2D(2,2)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(288, activation='relu')\n",
    "        self.dense2 = layers.Dense(8, activation='linear')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, model, env, gamma=0.85, mem_size=1000000, batch_size=64, expl_max=1.0, expl_min=0.01, expl_decay=0.995):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.memory = deque(maxlen=mem_size)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.expl_max = expl_max\n",
    "        self.expl_min = expl_min\n",
    "        self.expl_decay = expl_decay\n",
    "        self.expl_rate = expl_max\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for e in range(epochs):\n",
    "            s = self.env.reset()\n",
    "            s = np.array(s[None, :], dtype=np.float16)\n",
    "            d = False\n",
    "            while not d:\n",
    "                env.render()\n",
    "                a = self.action(s)\n",
    "                drive = self.drive(a)\n",
    "                sn, r, d, _ = env.step(drive)\n",
    "                sn = np.array(sn[None, :], dtype=np.float16)\n",
    "                self.remember(s, a, r, sn, d)\n",
    "                s = sn\n",
    "                self.experience_replay()\n",
    "\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for s, a, r, sn, d in batch:\n",
    "            q_update = r\n",
    "            if not d:\n",
    "                q_update = (r + self.gamma * np.amax(self.model.predict(sn)[0]))\n",
    "            q_values = self.model.predict(s)\n",
    "            q_values[0][a] = q_update\n",
    "            self.model.fit(s, q_values, verbose=0)\n",
    "        self.exploration_rate *= self.expl_decay\n",
    "        self.exploration_rate = max(self.expl_min, self.exploration_rate)\n",
    "\n",
    "    def remember(self, s, a, r, sn, d):\n",
    "        self.memory.append((s, a, r, sn, d))\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.rand() < self.expl_rate:\n",
    "            return random.randrange(1,4) #Tilfeldig innebærer alltid å kjøre slik at den lærer fort at den må bruke gassen for å komme fremover\n",
    "        return np.argmax(self.model(state))\n",
    "        \n",
    "    def drive(self, a):\n",
    "        if a == 1: #Full gass\n",
    "            return [0.0,1.0,0.0]\n",
    "        elif a == 2: #Full gass og sving høyre\n",
    "            return [1.0,1.0,0.0]\n",
    "        elif a == 3: #Full gass og sving venstre\n",
    "            return [-1.0,1.0,0.0]\n",
    "        elif a == 4: #Sving høyre\n",
    "            return [1.0,0.0,0.0]\n",
    "        elif a == 5: #Sving venstre\n",
    "            return [-1.0,0.0,0.0]\n",
    "        elif a == 6: #Full brems\n",
    "            return [0.0,0.0,1.0]\n",
    "        elif a == 7: #Full brems og sving høyre\n",
    "            return [1.0,0.0,1.0]\n",
    "        elif a == 8: #Full brems og sving venstre\n",
    "            return [-1.0,0.0,1.0]\n",
    "        else: #a==0 Gjør ingenting\n",
    "            return [0.0,0.0,0.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Track generation: 1273..1595 -> 322-tiles track\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m_ctypes/callbacks.c\u001b[0m in \u001b[0;36m'calling callback function'\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(hwnd, msg, wParam, lParam)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_window_proc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_handlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwnd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwParam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[0mevent_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevent_handlers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TokyoDrifter()\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "env = gym.make(\"CarRacing-v0\")\n",
    "\n",
    "dqn = DQN(model, env)\n",
    "\n",
    "dqn.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}